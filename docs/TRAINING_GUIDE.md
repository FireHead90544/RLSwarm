# Neural Swarm PPO - Complete Training Guide

## ğŸ¯ Project Overview

Multi-agent foraging simulation using **Proximal Policy Optimization (PPO)** with **Stable Baselines3**. Agents learn to cooperatively forage for food, transport to nest, and avoid collisions through shared policy learning.

---

## âœ… What Was Built

### Environment Features
- **Multi-Agent Physics**: 5 agents with autonomous movement, rotation, collision dynamics
- **Foraging Mechanics**: Food pickup, transport, deposit at nest
- **Optimized Reward Structure**:
  - Food pickup: **+5.0**
  - Food deposit: **+10.0** (2x pickup incentivizes completion)
  - Wall collision: **-0.5** (increased from -0.1 after initial training)
  - Agent collision: **-0.25** (allows cooperative clustering)
  - Step penalty: **-0.01** (10x stronger than initial for efficiency)

### Observation Space (15 dimensions per agent)
| Component | Dims | Description |
|-----------|------|-------------|
| **Raycasts** | 5 | Distance to walls/agents in 90Â° cone (0-1) |
| **Nearest Food** | 3 | Distance + direction vector (x, y) |
| **Self State** | 2 | Speed + carrying food flag |
| **Nest Location** | 3 | Distance + direction vector (x, y) |
| **Nearest Agent** | 2 | Distance + bearing (-1 to +1) |

### Action Space (5 discrete actions)
- 0: No-op
- 1: Rotate left (3Â°/step)
- 2: Rotate right (3Â°/step)
- 3: Accelerate (+0.2, max 5.0)
- 4: Decelerate (-0.2, min 0.0)

### Training Configuration
- **Algorithm**: PPO (Stable Baselines3)
- **Policy**: MlpPolicy (shared across agents)
- **Learning Rate**: 3e-4 (initial), 1e-4 (fine-tuning)
- **Batch Size**: 64
- **Rollout Steps**: 2048
- **Episodes**: Truncate after 2500 steps
- **Checkpoints**: Every 10,000 steps

---

## ğŸš€ Training Workflow

### 1. Fresh Training
```bash
# Default 1M timesteps
uv run train.py

# Custom timesteps
uv run train.py --steps 2000000
```

### 2. Continued Training from Checkpoint
```bash
# Continue from specific checkpoint
uv run train.py \
  --checkpoint models/ppo_swarm_5000000_steps.zip \
  --steps 1000000

# Timestep counter continues from checkpoint
# Logs append to TensorBoard automatically
```

### 3. Visualization
```bash
# View trained model (auto-enables debug HUD)
uv run play.py --model models/ppo_swarm_final.zip

# Adjust FPS for smoother playback
uv run play.py --model models/ppo_swarm_10621120_steps.zip --fps 60

# Controls:
# - D: Toggle debug HUD
# - TAB: Switch selected agent
# - R: Reset environment
# - ESC: Quit
```

### 4. Monitor Training (TensorBoard)
```bash
tensorboard --logdir logs/

# Key metrics:
# - rollout/ep_rew_mean: Episode rewards (should increase)
# - rollout/ep_len_mean: Episode length (~2500)
# - train/entropy_loss: Exploration (decreases as policy stabilizes)
# - train/approx_kl: Policy update size
# - train/explained_variance: Value function quality
```

---

## ğŸ“Š Training Results & Insights

### Convergence Timeline
| Steps | ep_rew_mean | Behavior |
|-------|-------------|----------|
| 0-1M | -20 to 0 | Random exploration, learning navigation |
| 1M-3M | 0 to +10 | Food pickup learned, occasional deposits |
| 3M-5M | +10 to +16 | Efficient foraging cycles, some collisions |
| 5M-7M | +13 to +20* | Fine-tuning with increased wall penalty |
| 7M-10M | +20 to +30* | Minimal collisions, optimal paths |

*After wall collision penalty increased to -0.5

### Successful Training Metrics (at 10.6M steps)
```
rollout/ep_rew_mean:     ~25-30
rollout/ep_len_mean:     2500 (max)
train/approx_kl:         0.011 (stable)
train/entropy_loss:      -1.09 (deterministic policy)
train/explained_variance: 0.70-0.75 (good value estimation)
```

### Observed Behaviors âœ…
- âœ… **Foraging**: Actively seek and pickup food
- âœ… **Depositing**: Navigate to nest and deposit
- âœ… **Efficiency**: Minimize wasted steps (step penalty working)
- âœ… **Agent Avoidance**: Occasional collisions, mostly avoid
- âœ… **Wall Avoidance**: Minimal wall hits after reward tuning
- âœ… **Cooperation**: Natural clustering near food sources

---

## ğŸ”§ Reward Tuning Journey

### Initial Configuration (Steps 0-5M)
```python
REWARD_WALL_COLLISION = -0.1
REWARD_AGENT_COLLISION = -0.5
REWARD_STEP = -0.001
```
**Issue**: Wall collisions not penalized enough (50x weaker than food rewards)

### Optimized Configuration (Steps 5M+)
```python
REWARD_WALL_COLLISION = -0.5   # 5x increase
REWARD_AGENT_COLLISION = -0.25 # Reduced 50% to allow clustering
REWARD_STEP = -0.01            # 10x increase for efficiency
```
**Result**: Significant reduction in wall collisions, maintained cooperative behavior

### Why These Changes Work
- **Wall collision (-0.5)**: Now hitting 10 walls = losing 1 food pickup
- **Agent collision (-0.25)**: Allows natural clustering without excessive avoidance
- **Step penalty (-0.01)**: Creates urgency (500 wasted steps = 1 food pickup lost)

---

## ğŸ“ Training Best Practices

### 1. Start with Default Rewards
- Let agents learn core task (foraging/depositing) first
- Wait for convergence (3-5M steps)

### 2. Identify Behavioral Issues
- Use TensorBoard metrics
- Visualize with `play.py`
- Look for repeated mistakes (e.g., wall collisions)

### 3. Adjust Rewards Gradually
- Increase penalties 2-5x at a time
- Continue from best checkpoint
- Monitor for sudden performance drops

### 4. Fine-Tune with Lower Learning Rate
```bash
# Manually edit train.py or use:
# learning_rate = 1e-4  (down from 3e-4)
```

### 5. Track Multiple Checkpoints
- Save every 10k steps
- Test various checkpoints with `play.py`
- Choose best based on visual behavior + metrics

---

## ğŸ› Common Issues & Solutions

### Issue: Episodes Never End (No TensorBoard Metrics)
**Symptom**: `rollout/ep_rew_mean` not showing
**Solution**: Episodes now truncate after 2500 steps âœ…

### Issue: Training Plateau
**Symptom**: `ep_rew_mean` flat for 1M+ steps
**Solutions**:
- Increase reward magnitude (2-5x)
- Reduce learning rate (1e-4 or 5e-5)
- Check if task is already solved
- Add curriculum learning (harder obstacles)

### Issue: Agents Collide Too Much
**Solutions**:
- Increase collision penalty (try -0.5 to -1.0)
- Check ray coverage (90Â° might be too narrow)
- Increase entropy coefficient (more exploration)

### Issue: Slow Training
**Solutions**:
- Use CPU for PPO+MlpPolicy (faster than GPU!)
- Reduce `n_steps` to 1024 (less memory, more frequent updates)
- Decrease `batch_size` to 32

### Issue: Lag in play.py
**Solution**: Reduce FPS with `--fps 30` âœ…

---

## ğŸ”¬ Advanced Experimentation

### Hyperparameter Tuning
```python
# In train.py, try:
learning_rate = 1e-4 to 5e-4
n_steps = 1024 to 4096
batch_size = 32 to 128
ent_coef = 0.001 to 0.05
```

### Environment Variations
```python
# In config.py:
NUM_AGENTS = 10        # Scale up swarm
MAX_FOOD = 50          # More food sources
REWARD_STEP = -0.02    # Even more efficiency pressure
```

### Multi-Stage Training
1. Train 2M steps with easy rewards
2. Increase penalties progressively
3. Fine-tune with low learning rate

---

## ğŸ“ Project Structure

```
neural_swarm_ppo/
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ swarm_env.py       # Core Gymnasium environment
â”‚   â””â”€â”€ wrappers.py        # SwarmVecEnv wrapper for SB3
â”œâ”€â”€ models/                # Checkpoints saved here
â”‚   â””â”€â”€ ppo_swarm_*.zip
â”œâ”€â”€ logs/                  # TensorBoard logs
â”‚   â””â”€â”€ PPO_*/
â”œâ”€â”€ config.py              # All hyperparameters
â”œâ”€â”€ train.py               # Training script (with checkpoint support)
â”œâ”€â”€ play.py                # Visualization tool
â”œâ”€â”€ manual_control.py      # Manual testing
â”œâ”€â”€ check_env.py           # Environment validation
â”œâ”€â”€ TRAINING_GUIDE.md      # This file
â””â”€â”€ README.md              # Quick reference
```

---

## ğŸ¯ Google Colab Training

### Setup
```python
# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Extract project
!unzip -q /content/drive/MyDrive/neural_swarm_ppo.zip -d /content/
%cd /content/neural_swarm_ppo

# Install dependencies
!pip install -q stable-baselines3[extra] shimmy gymnasium pygame tensorboard

# Verify GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
```
### Monitor (Start TensorBoard)
```python
%load_ext tensorboard
%tensorboard --logdir logs/
```

### Training
```python
# Fresh start
!python train.py --steps 2000000

# Continue from checkpoint
!python train.py \
  --checkpoint models/ppo_swarm_5000000_steps.zip \
  --steps 2000000
```

### Download Results
```python
!zip -r results.zip models/ logs/
!cp results.zip /content/drive/MyDrive/
```

---

## ğŸ† Success Criteria

### Training Complete When:
- âœ… `ep_rew_mean` > 20 for 500k+ steps
- âœ… Visual inspection shows efficient foraging
- âœ… Wall collisions < 2 per episode
- âœ… Food deposit rate > 3 per episode
- âœ… `explained_variance` > 0.65

### Your Results (10.6M steps):
- âœ… `ep_rew_mean`: ~25-30
- âœ… Minimal collisions
- âœ… Efficient foraging cycles
- âœ… Cooperative clustering
- âœ… **Training successful!**

---

## ğŸ“– Key Learnings

1. **Start Simple**: Basic rewards first, tune later
2. **Visualize Often**: `play.py` reveals issues metrics miss
3. **Iterate on Rewards**: Don't be afraid to 5x a penalty
4. **Trust Convergence**: Plateaus are normal after 3-5M steps
5. **Use Checkpoints**: Test multiple saves to find best model
6. **Episode Limits Matter**: Without truncation, no TensorBoard metrics
7. **GPU Not Always Better**: PPO+MLP runs faster on CPU

---

## ğŸš€ Next Steps

1. **Experiment with metrics/graphs** (you're doing this tomorrow!)
2. **Try different agent counts** (10-20 agents)
3. **Add obstacles** to environment
4. **Implement evaluation callback** for best model selection
5. **Test transfer learning** (pretrain on simple, transfer to complex)
6. **Analyze emergent behaviors** in long episodes

---

## ğŸ‰ Congratulations!

You've successfully:
- âœ… Built a custom Gymnasium environment
- âœ… Trained PPO agents for 10M+ steps
- âœ… Diagnosed and fixed reward scaling issues
- âœ… Achieved efficient cooperative foraging
- âœ… Implemented checkpoint-based training
- âœ… Created reproducible training pipeline

Your swarm is ready for research! ğŸ

---

## ğŸ“š References

- [Stable Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gymnasium API](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Multi-Agent RL Survey](https://arxiv.org/abs/1911.10635)

---

**Last Updated**: 2025-11-27  
**Training Status**: âœ… Converged and Optimized (Could be further improved with more tuning) 
**Best Model**: `ppo_swarm_9771120_steps_reshaped.zip` to `ppo_swarm_10621120_steps_reshaped.zip`
